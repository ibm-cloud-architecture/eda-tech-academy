<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="Jerome Boyer"><link href=../getting-started/mm2/ rel=prev><link href=../lab1/ rel=next><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.4.2, mkdocs-material-9.0.6"><title>Demonstration A to Z - EDA technical academy</title><link rel=stylesheet href=../assets/stylesheets/main.558e4712.min.css><link rel=stylesheet href=../assets/stylesheets/palette.2505c338.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../extra.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary data-md-color-accent> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#demonstrating-event-streams-from-a-to-z class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=.. title="EDA technical academy" class="md-header__button md-logo" aria-label="EDA technical academy" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> EDA technical academy </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Demonstration A to Z </span> </div> </div> </div> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/ibm-cloud-architecture/eda-tech-academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class="md-tabs__inner md-grid"> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=.. class=md-tabs__link> Introduction </a> </li> <li class=md-tabs__item> <a href=../eda/ class=md-tabs__link> Information </a> </li> <li class=md-tabs__item> <a href=../getting-started/ class="md-tabs__link md-tabs__link--active"> Journey 1 </a> </li> <li class=md-tabs__item> <a href=../lab1/ class=md-tabs__link> Journey 2 </a> </li> <li class=md-tabs__item> <a href=../future/ class=md-tabs__link> More... </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=.. title="EDA technical academy" class="md-nav__button md-logo" aria-label="EDA technical academy" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg> </a> EDA technical academy </label> <div class=md-nav__source> <a href=https://github.com/ibm-cloud-architecture/eda-tech-academy title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.2.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> GitHub </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=.. class=md-nav__link> Introduction </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle=__nav_2 type=checkbox id=__nav_2> <label class=md-nav__link for=__nav_2 tabindex=0 aria-expanded=false> Information <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label=Information data-md-level=1> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Information </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../eda/ class=md-nav__link> Why EDA now? </a> </li> <li class=md-nav__item> <a href=https://ibm-cloud-architecture.github.io/refarch-eda/concepts/fit-to-purpose/ class=md-nav__link> Fit for purpose </a> </li> <li class=md-nav__item> <a href=https://ibm-cloud-architecture.github.io/refarch-eda/introduction/reference-architecture/#event-driven-architecture class=md-nav__link> EDA Reference Architecture </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " data-md-toggle=__nav_3 type=checkbox id=__nav_3 checked> <label class=md-nav__link for=__nav_3 tabindex=0 aria-expanded=true> Journey 1 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Journey 1" data-md-level=1> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Journey 1 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../getting-started/ class=md-nav__link> Getting started with Event Streams </a> </li> <li class=md-nav__item> <a href=../getting-started/schema-lab/ class=md-nav__link> Event Streams schema </a> </li> <li class=md-nav__item> <a href=../getting-started/eepm/ class=md-nav__link> Event-endpoint management </a> </li> <li class=md-nav__item> <a href=../getting-started/mm2/ class=md-nav__link> Mirror maker 2 lab </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" data-md-toggle=toc type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> Demonstration A to Z <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> Demonstration A to Z </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#pre-requisites class=md-nav__link> Pre-requisites </a> </li> <li class=md-nav__item> <a href=#review-event-streams-components class=md-nav__link> Review Event Streams components </a> </li> <li class=md-nav__item> <a href=#concepts class=md-nav__link> Concepts </a> </li> <li class=md-nav__item> <a href=#high-availability class=md-nav__link> High Availability </a> </li> <li class=md-nav__item> <a href=#operator-based-deployment class=md-nav__link> Operator based deployment </a> </li> <li class=md-nav__item> <a href=#review-event-streams-user-interface-features class=md-nav__link> Review Event Streams user interface features </a> <nav class=md-nav aria-label="Review Event Streams user interface features"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#topic-management class=md-nav__link> Topic management </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#run-the-starter-application class=md-nav__link> Run the Starter Application </a> </li> <li class=md-nav__item> <a href=#back-to-the-cluster-configuration class=md-nav__link> Back to the Cluster configuration </a> </li> <li class=md-nav__item> <a href=#producing-messages class=md-nav__link> Producing messages </a> </li> <li class=md-nav__item> <a href=#schema-registry class=md-nav__link> Schema registry </a> </li> <li class=md-nav__item> <a href=#consumer-application-consumer-group class=md-nav__link> Consumer application - consumer group </a> </li> <li class=md-nav__item> <a href=#user-management-and-security class=md-nav__link> User management and security </a> </li> <li class=md-nav__item> <a href=#kafka-connect class=md-nav__link> Kafka Connect </a> </li> <li class=md-nav__item> <a href=#monitoring class=md-nav__link> Monitoring </a> </li> <li class=md-nav__item> <a href=#event-streaming class=md-nav__link> Event Streaming </a> <nav class=md-nav aria-label="Event Streaming"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#kafka-streams class=md-nav__link> Kafka Streams </a> </li> <li class=md-nav__item> <a href=#apache-flink-as-your-streaming-platform class=md-nav__link> Apache Flink as your streaming platform </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#real-time-inventory-demo class=md-nav__link> Real-time inventory demo </a> </li> <li class=md-nav__item> <a href=#geo-replication class=md-nav__link> Geo-replication </a> <nav class=md-nav aria-label=Geo-replication> <ul class=md-nav__list> <li class=md-nav__item> <a href=#demonstrating-geo-replication class=md-nav__link> Demonstrating Geo Replication </a> </li> <li class=md-nav__item> <a href=#mirror-maker-2 class=md-nav__link> Mirror Maker 2 </a> </li> <li class=md-nav__item> <a href=#active-passive class=md-nav__link> Active - Passive </a> </li> <li class=md-nav__item> <a href=#active-active class=md-nav__link> Active - Active </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#day-2-operations class=md-nav__link> Day 2 operations </a> <nav class=md-nav aria-label="Day 2 operations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gitops class=md-nav__link> GitOps </a> </li> <li class=md-nav__item> <a href=#start-the-gitops-demo class=md-nav__link> Start the GitOps demo </a> </li> <li class=md-nav__item> <a href=#event-streams-cluster-definition-with-gitops class=md-nav__link> Event Streams cluster definition with GitOps </a> </li> <li class=md-nav__item> <a href=#event-streams-cluster-upgrade class=md-nav__link> Event Streams Cluster upgrade </a> </li> <li class=md-nav__item> <a href=#topic-management-with-gitops class=md-nav__link> Topic management with GitOps </a> </li> <li class=md-nav__item> <a href=#repartitioning class=md-nav__link> Repartitioning </a> </li> <li class=md-nav__item> <a href=#clean-your-gitops class=md-nav__link> Clean your gitops </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#openshift-cluster-version-upgrade class=md-nav__link> OpenShift Cluster version upgrade </a> <nav class=md-nav aria-label="OpenShift Cluster version upgrade"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#principles class=md-nav__link> Principles </a> </li> <li class=md-nav__item> <a href=#what-can-be-demonstrated class=md-nav__link> What can be demonstrated </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle=__nav_4 type=checkbox id=__nav_4> <label class=md-nav__link for=__nav_4 tabindex=0 aria-expanded=false> Journey 2 <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Journey 2" data-md-level=1> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Journey 2 </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle=__nav_4_1 type=checkbox id=__nav_4_1> <label class=md-nav__link for=__nav_4_1 tabindex=0 aria-expanded=false> Lab 1 - System Design Lab <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Lab 1 - System Design Lab" data-md-level=2> <label class=md-nav__title for=__nav_4_1> <span class="md-nav__icon md-icon"></span> Lab 1 - System Design Lab </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../lab1/ class=md-nav__link> Problem Statement </a> </li> <li class=md-nav__item> <a href=../lab1/lab1-sol/ class=md-nav__link> Solution Overview </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" data-md-toggle=__nav_4_2 type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 tabindex=0 aria-expanded=false> Kafka Streams Lab <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav aria-label="Kafka Streams Lab" data-md-level=2> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> Kafka Streams Lab </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../lab2/kstream/ class=md-nav__link> Kafka Streams Study </a> </li> <li class=md-nav__item> <a href=../lab2/ class=md-nav__link> Lab 2 exercises </a> </li> <li class=md-nav__item> <a href=../lab2/lab2-sol/ class=md-nav__link> Solution Overview </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../lab3/ class=md-nav__link> Lab 3 - Real Time Inventory demo </a> </li> <li class=md-nav__item> <a href=../lab4/ class=md-nav__link> Lab 4 - EDA GitOps </a> </li> <li class=md-nav__item> <a href=../lab5/ class=md-nav__link> Lab 5 - Monitoring with Instana </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../future/ class=md-nav__link> More... </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#pre-requisites class=md-nav__link> Pre-requisites </a> </li> <li class=md-nav__item> <a href=#review-event-streams-components class=md-nav__link> Review Event Streams components </a> </li> <li class=md-nav__item> <a href=#concepts class=md-nav__link> Concepts </a> </li> <li class=md-nav__item> <a href=#high-availability class=md-nav__link> High Availability </a> </li> <li class=md-nav__item> <a href=#operator-based-deployment class=md-nav__link> Operator based deployment </a> </li> <li class=md-nav__item> <a href=#review-event-streams-user-interface-features class=md-nav__link> Review Event Streams user interface features </a> <nav class=md-nav aria-label="Review Event Streams user interface features"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#topic-management class=md-nav__link> Topic management </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#run-the-starter-application class=md-nav__link> Run the Starter Application </a> </li> <li class=md-nav__item> <a href=#back-to-the-cluster-configuration class=md-nav__link> Back to the Cluster configuration </a> </li> <li class=md-nav__item> <a href=#producing-messages class=md-nav__link> Producing messages </a> </li> <li class=md-nav__item> <a href=#schema-registry class=md-nav__link> Schema registry </a> </li> <li class=md-nav__item> <a href=#consumer-application-consumer-group class=md-nav__link> Consumer application - consumer group </a> </li> <li class=md-nav__item> <a href=#user-management-and-security class=md-nav__link> User management and security </a> </li> <li class=md-nav__item> <a href=#kafka-connect class=md-nav__link> Kafka Connect </a> </li> <li class=md-nav__item> <a href=#monitoring class=md-nav__link> Monitoring </a> </li> <li class=md-nav__item> <a href=#event-streaming class=md-nav__link> Event Streaming </a> <nav class=md-nav aria-label="Event Streaming"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#kafka-streams class=md-nav__link> Kafka Streams </a> </li> <li class=md-nav__item> <a href=#apache-flink-as-your-streaming-platform class=md-nav__link> Apache Flink as your streaming platform </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#real-time-inventory-demo class=md-nav__link> Real-time inventory demo </a> </li> <li class=md-nav__item> <a href=#geo-replication class=md-nav__link> Geo-replication </a> <nav class=md-nav aria-label=Geo-replication> <ul class=md-nav__list> <li class=md-nav__item> <a href=#demonstrating-geo-replication class=md-nav__link> Demonstrating Geo Replication </a> </li> <li class=md-nav__item> <a href=#mirror-maker-2 class=md-nav__link> Mirror Maker 2 </a> </li> <li class=md-nav__item> <a href=#active-passive class=md-nav__link> Active - Passive </a> </li> <li class=md-nav__item> <a href=#active-active class=md-nav__link> Active - Active </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#day-2-operations class=md-nav__link> Day 2 operations </a> <nav class=md-nav aria-label="Day 2 operations"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#gitops class=md-nav__link> GitOps </a> </li> <li class=md-nav__item> <a href=#start-the-gitops-demo class=md-nav__link> Start the GitOps demo </a> </li> <li class=md-nav__item> <a href=#event-streams-cluster-definition-with-gitops class=md-nav__link> Event Streams cluster definition with GitOps </a> </li> <li class=md-nav__item> <a href=#event-streams-cluster-upgrade class=md-nav__link> Event Streams Cluster upgrade </a> </li> <li class=md-nav__item> <a href=#topic-management-with-gitops class=md-nav__link> Topic management with GitOps </a> </li> <li class=md-nav__item> <a href=#repartitioning class=md-nav__link> Repartitioning </a> </li> <li class=md-nav__item> <a href=#clean-your-gitops class=md-nav__link> Clean your gitops </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#openshift-cluster-version-upgrade class=md-nav__link> OpenShift Cluster version upgrade </a> <nav class=md-nav aria-label="OpenShift Cluster version upgrade"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#principles class=md-nav__link> Principles </a> </li> <li class=md-nav__item> <a href=#what-can-be-demonstrated class=md-nav__link> What can be demonstrated </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <h1 id=demonstrating-event-streams-from-a-to-z>Demonstrating Event Streams from A to Z<a class=headerlink href=#demonstrating-event-streams-from-a-to-z title="Permanent link">&para;</a></h1> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>This exercise is not a step by step lab, but more an explanation of all the concepts and components involved in an event-driven solution with Event Streams. We have provided scripts that can be leveraged (see the table of content on the right to get scripts) to demonstrate and talk about those items in front of your prospect. </p> </div> <p>A typical demonstration script will include at least the following subjects (See right navigation bar to go to a specific sections):</p> <ul> <li>Review Event Streams Components</li> <li>Operator based deployment and Day 2 operations</li> <li>Topic creation</li> <li>Producer application</li> <li>Consumer application, consumer group concepts, offset concepts</li> <li>User access, authentication mechanism</li> <li>Monitoring</li> <li>Event Streaming</li> <li>Geo-replication</li> </ul> <p>As education enablement you can go step by step following the current structure. As a reusable asset for your future demonstration, you can pick and choose from the right navigation bar the items to highlight in front of your audiance.</p> <p>All the demonstration can be done on IBM CoC clusters: see the <a href=../#environments>environments</a> section in the EDA labs introduction.</p> <h2 id=pre-requisites>Pre-requisites<a class=headerlink href=#pre-requisites title="Permanent link">&para;</a></h2> <p>You will need access to an Event Streams instance installed on an OpenShift cluster with access to the OpenShift Console to demonstrate Operators. </p> <p>You’ll need the following as well:</p> <ul> <li>git client</li> <li> <p>Have <a href=https://docs.openshift.com/container-platform/4.7/cli_reference/openshift_cli/getting-started-cli.html>oc cli</a> installed. It can be done once connected to the OpenShift cluster using the &lt;?&gt; icon on the top-right and "Command Line Tool" menu.</p> <p><img alt src=images/access-oc-cli.png></p> </li> <li> <p>Get <a href=https://www.docker.com/products/docker-desktop/ >docker desktop</a> or <a href=https://podman.io/ >podman</a> on your local laptop</p> </li> <li>Java 11 is need to run the <a href=./#run-the-starter-application>Event Streams starter application</a>.</li> </ul> <h2 id=review-event-streams-components>Review Event Streams components<a class=headerlink href=#review-event-streams-components title="Permanent link">&para;</a></h2> <p><strong>Narative</strong>: Event Streams is the IBM packaging of different Open Source projects to support an integrated user experience deploying and managing Kafka on OpenShift cluster. The following figure illustrates such components:</p> <p><img alt src=images/es-components.png></p> <p><strong><a href=https://github.com/ibm-cloud-architecture/eda-gitops-catalog/blob/main/docs/diagrams/es-components.drawio>src for this diagram is here</a></strong></p> <ul> <li>Event streams (<a href=https://kafka.apache.org>Apache Kafka</a> packaging) runs on OpenShift cluster.</li> <li>The deployment and the continuous monitoring of Event Streams resources definition and deployed resources is done via Operator (<a href=http://strimzi.io/ >Strimzi open source project</a>)</li> <li>Event Streams offers a user interface to manage resources and exposes simple dashboard. We will use it during the demonstration.</li> <li>The schema management is done via schema registry and the feature is integrated in Event Streams user interface but in the back end, is supported by <a href=http://apicur.io/registry>Apicur.io registry</a></li> <li>External event sources can be integrated via the <a href=https://kafka.apache.org/documentation/#connect>Kafka Connector framework</a> and Event Streams offers <a href=https://ibm.github.io/event-streams/connectors/ >a set of connectors</a> and can partner to other companies to get specific connectors.</li> <li>External sinks can be used to persist messages for longer time period that the retention settings done at the topic level. S3 buckets can be use, IBM Cloud object storage, and Kafka Sink connectors. There is <a href=https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/connect-cos/ >this cloud object storage lab</a>, or <a href=https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/connect-s3/ >S3 sink with Apache Camel lab</a> to present such integrations.</li> <li>Event Streams monitoring is done using Dashboards in Event Streams user interface but also within OpenShift monitoring and Kibana dashboards.</li> <li>Green components are application specifics, and represent event-driven microservices (see <a href=https://github.com/ibm-cloud-architecture/eda-quickstarts>eda-quickstart project for code templates</a>) or Kafka Streaming apps, or <a href=https://flink.apache.org/ >Apache Flink</a> apps.</li> <li>For cluster optimization, Event Streams integrates Cruise Control, with goal constraints, to act on cluster resource usage.</li> </ul> <details class=-> <summary>More argumentations</summary> <ul> <li>Kafka is essentially a distributed platform to manage append log with a pub/sub protocol to get streams of events. Messages are saved for a long period of time.</li> <li>Kafka connectors can also being supported by APP Connect integration capabilities or <a href=https://camel.apache.org/camel-kafka-connector/1.0.x/reference/index.html>Apache Camel kafka connectors</a>.</li> <li>To learn more about <a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-connect/ >Kafka Connector see our summary</a></li> </ul> </details> <h2 id=concepts>Concepts<a class=headerlink href=#concepts title="Permanent link">&para;</a></h2> <p>If needed there are some important concepts around Kafka to present to your audience. <a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/ >See this kafka technology overview</a>.</p> <h2 id=high-availability>High Availability<a class=headerlink href=#high-availability title="Permanent link">&para;</a></h2> <p>High availability is ensured by avoiding single point of failure, parallel, and replications. The following figure is a golden topology for OpenShift with Event Streams components deployed to it. Event Streams Brokers run in OpenShift worker nodes, and it may be relevant to use one broker per worker nodes using zone affinity policies. </p> <p><img alt src=images/es-golden-topo.png> <strong><a href=https://github.com/ibm-cloud-architecture/eda-gitops-catalog/blob/main/docs/diagrams/es-golden-topo.drawio>src for this diagram is here</a></strong></p> <p>Kafka connectors, or streaming applications runs in worker node too and access brokers via mutual TLS authentication and SSL encryption.</p> <p>Kafka brokers are spread across worker nodes using anti-affinity policies.</p> <details class=-> <summary>Read more</summary> <ul> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/advance/#high-availability>Kafka High availability deeper dive</a></li> <li>See the <a href=https://production-gitops.dev/infrastructure/golden-topology/ >OpenShift golden topology article</a> in production deployment site.</li> <li>A production <a href=https://github.com/ibm-cloud-architecture/eda-gitops-catalog/blob/main/cp4i-operators/event-streams/operands/prod-small/eventstreams-prod.yaml>deployment descriptor for Event Streams</a></li> <li><a href=https://ibm.github.io/event-streams/installing/planning/ >Product documentation on planning installation</a></li> </ul> </details> <h2 id=operator-based-deployment>Operator based deployment<a class=headerlink href=#operator-based-deployment title="Permanent link">&para;</a></h2> <p>There are several ways to install Event Streams. We are going to look at this, with Operator Hub. </p> <ol> <li> <p>Go to your Openshift console, select Operator Hub and search for Event Streams. Here you can install the operator to manage all cluster instances deployed to the OpenShift environment. Operator can automatically deploy new product version once released by IBM.</p> <p><img alt src=images/operatorHub.png></p> </li> <li> <p>In the OpenShift Console, select the project where Event Streams is deployed. On left menu select <code>Operators &gt; Installed Operators</code>, scroll to select IBM Event Streams, you are now in the Operator user interface, from where you can see local resources and create new one.</p> <p><img alt src=images/es-operator-home.png></p> </li> <li> <p>Go to the <code>Event Streams</code> menu and select existing cluster definition</p> <p><img alt src=images/es-demo-operands.png></p> </li> <li> <p>You are now viewing the cluster definition as it is deployed. Select the <code>YAML</code> choice and see the <code>spec</code> elements. </p> <p><img alt src=images/es-yaml-view.png></p> <p>You can see how easy it would be simple to add a broker by changing the <code>spec.strimziOverrides.kafka.replicas</code> value. Also in this view, the <code>Samples</code> menu presents some examples of cluster definitions. Kafka brokers, Zookeeper nodes or other components like Apicurio can all be scaled to meet your needs: </p> <ul> <li>Number of replicas</li> <li>CPU request or limit settings</li> <li>Memory request or limit settings</li> <li>JVM settings</li> </ul> </li> <li> <p>On the left side menu select Workloads-&gt;Pods. Here you see pods that are in the Event Streams namespace like Broker, zookeepers, user interface, schema registry:</p> <p><img alt src=images/pods.png></p> </li> <li> <p>If needed, you can explain the concept of persistence and Storage class: Kafka save records on disk for each broker, and so it can use VM disk or network file systems. As Kubernetes deployed application, Event Streams define persistence via persistence claim and expected quality of service using storage class.</p> <p><img alt src=images/persistance.png></p> <p>On the left side menu select, <code>Storage &gt; PersistenceVolumesClaims</code> in the OpenShift console, each broker has its own claim, OpenShift allocated Persistence Volumes with expected capacity. The Storage class was defined by OpenShift administrator, and in the example above, it use CEPH storage.</p> </li> </ol> <details class=-> <summary>Read more</summary> <ul> <li><a href=https://docs.ceph.com/en/latest/rbd/rbd-kubernetes/ >Cepth and block devise</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/#kafka-components>Kafka Brokers</a> and architecture</li> <li><a href=#day-2-operations>GitOps approach for Day1 and Day 2 operations</a></li> </ul> </details> <h2 id=review-event-streams-user-interface-features>Review Event Streams user interface features<a class=headerlink href=#review-event-streams-user-interface-features title="Permanent link">&para;</a></h2> <p>There are a number of ways to navigate to Event Streams Console by getting the exposed routes</p> <ol> <li> <p>Using Routes in Openshift: On the left side menu select <code>Networking &gt; Routes</code> in the OpenShift console. Find <code>es-demo-ibm-es-ui</code> and then go to Location. Select that link and it will take you to the Event Streams Console. Depending to your installation, you may reach Cloud Pak for Integration console, in this case, select Entreprise LDAP, and enter your userid and password.</p> <p><img alt src=images/cpi-prompt.png></p> </li> <li> <p>Using the cli: (replace <code>es-demo</code> with the name of your cluster, and <code>cp4i-eventstreams</code> with the name of the project where Event Streams runs into )</p> <div class=highlight><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>chrome<span class=w> </span><span class=k>$(</span>oc<span class=w> </span>get<span class=w> </span>eventstreams<span class=w> </span>es-demo<span class=w> </span>-n<span class=w> </span>cp4i-eventstreams<span class=w> </span>-o<span class=w> </span><span class=nv>jsonpath</span><span class=o>=</span><span class=s1>&#39;{.status.adminUiUrl}&#39;</span><span class=k>)</span>
</code></pre></div> <p>Once you logged in using the LDAP credentials provided, you should reach the home page.</p> <p><img alt src=images/es-console.png></p> </li> <li> <p>The set of features available from this home page, are <strong>topic management, schema registry, consumer groups, monitoring, and toolbox</strong>... you will review most of those features in this demo.</p> </li> </ol> <h3 id=topic-management>Topic management<a class=headerlink href=#topic-management title="Permanent link">&para;</a></h3> <p>Topics are append log, producer applications publish records to topics, and consumer applications subscribe to topics. Kafka messages themselves are immutable. Deletion and compaction of data are administrative operations.</p> <ol> <li> <p>Navigate to the topic main page by using the Event Streams left side menu and select Topics.</p> <p><img alt src=images/topic-main-page.png></p> <ul> <li><strong>replicas</strong> are to support record replication and to ensure high availability. Producer can wait to get acknowledgement of replication. Replicas needs to be set to 3 to supports 2 broker failures at the same time. </li> <li><strong>partition</strong> defines the number of append logs managed by the broker. Each partition has a leader, and then follower brokers that replicate records from the leader. Partitions are really done to do parallel processing at the consumer level. </li> <li>The following diagram can be used to explain those concepts. </li> </ul> <p><img alt src=images/topic-concept.png></p> </li> <li> <p>Create a topic for the <strong>Starter</strong> app, using the user interface:</p> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>When running on a multi-tenant Event Streams cluster you need to modify the name of the topic, to avoid conflicting with other topic name, use your userid as prefix.</p> </div> <p><img alt src=images/topic-name.png></p> <p>Use only one partition.</p> <p><img alt src=images/topic-partitions.png></p> <p>The default retention time is 7 days, Kafka is keeping data for a long time period, so any consumer applications can come and process messages at any time. It helps for microservice resilience and increase decoupling.</p> <p><img alt src=images/topic-message-retention.png></p> <p>Finally the replicas for high availability. 3 is the production deployment, and in-sync replicas = 2, means producer get full acknowledge when there are 2 replicas done. Broker partition leader keeps information of in-sync replicas.</p> <p><img alt src=images/topic-replicas.png></p> </li> <li> <p>Just as an important note, topic may be created via yaml file or using CLI command. Go to the <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/services/ibm-eventstreams/base/es-topics.yaml>rt-inventory GitOps - es-topics.yaml</a> and explain some of the parameters.</p> </li> <li> <p>We will go over the process of adding new topic by using GitOps in <a href=./#topic-management-with-gitops>this section</a></p> </li> </ol> <details class=-> <summary>Read more</summary> <ul> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/#topics>Topic summary</a></li> <li><a href=https://kafka.apache.org/documentation/#topicconfigs>Kafka topic configuration</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-producers/ >Understand Kafka producer</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-consumers/ >Review Consumer</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-overview/advance/#replication-and-partition-leadership>Replication and partition leadership</a></li> </ul> </details> <h2 id=run-the-starter-application>Run the Starter Application<a class=headerlink href=#run-the-starter-application title="Permanent link">&para;</a></h2> <p>See the <a href=../getting-started/ >beginned dedicated lab</a> to get the application started, once done:</p> <ol> <li> <p>Go back to the Event Streams console, Topic management, and the <code>starter-app</code> topic, select the <code>Messages</code> tab and go to any messages. Explain that each messages has a timestamp, and an offset that is an increasing number. Offset are used by consumer to be able to replay from an older message, or when restarting from a failure. Offset management at the consumer application level is tricky, if needed you can have a deeper conversation on this topic later after the demonstration.</p> <p><img alt src=images/starter-app-topic.png></p> </li> <li> <p>At the topic level, it is possible to see the consumer of the topic: Go to the <code>Consumer groups</code> tab, to see who are the consumer, if the consumer is active or not (this will be controlled by the heartbeat exchanges when consumer poll records and commit their read offset). </p> <p><img alt src=images/topic-consumer-group.png></p> <p>One important metric in this table is the unconsumed partition value. If the number of partitions is more than 1 and there are less than the number of consumer than of partition, then it means a consumer is processing two or more partitions.</p> </li> <li> <p>Going by to the starter application, you can start consuming the records. This is to demonstrate that consumer can connect at anytime, and that it will quickly consume all messages. Stopping and restarting is also demonstrating that consumer, continues from the last read offset.</p> </li> </ol> <p>There is an alternate of running this application on your laptop, it can be deployed directly to the same OpenShift cluster, we have defined <code>deployment and config map</code> to do so.</p> <details class=-> <summary>Deploy starter app on OpenShift</summary> <ul> <li>Use the same <code>kafka.properties</code> and <code>truststore.p12</code> files you have downloaded with the starter application to create two kubernetes secrets holding these files in your OpenShift cluster</li> </ul> <div class=highlight><pre><span></span><code><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>oc<span class=w> </span>create<span class=w> </span>secret<span class=w> </span>generic<span class=w> </span>demo-app-secret<span class=w> </span>--from-file<span class=o>=</span>./kafka.properties
<a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>oc<span class=w> </span>create<span class=w> </span>secret<span class=w> </span>generic<span class=w> </span>truststore-cert<span class=w> </span>--from-file<span class=o>=</span>./truststore.p12
</code></pre></div> <ul> <li>Clone the following GitHub repo that contains the Kubernetes artifacts that will run the starter application.</li> </ul> <div class=highlight><pre><span></span><code><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>git<span class=w> </span>clone<span class=w> </span>https://github.com/ibm-cloud-architecture/eda-quickstarts.git
</code></pre></div> <ul> <li>Change directory to where those Kubernetes artefacts are.</li> </ul> <div class=highlight><pre><span></span><code><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=nb>cd</span><span class=w> </span>eda-quickstarts/kafka-java-vertz-starter
</code></pre></div> <ul> <li>Deploy the Kubernetes artefacts.</li> </ul> <div class=highlight><pre><span></span><code><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a>oc<span class=w> </span>apply<span class=w> </span>-k<span class=w> </span>app-deployment
</code></pre></div> <ul> <li>Get the route to the starter application running on your OpenShift cluster.</li> </ul> <div class=highlight><pre><span></span><code><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>oc<span class=w> </span>get<span class=w> </span>route<span class=w> </span>es-demo<span class=w> </span>-o<span class=o>=</span><span class=nv>jsonpath</span><span class=o>=</span><span class=s1>&#39;{.status.ingress[].host}&#39;</span>
</code></pre></div> <ul> <li>Point your browser to that url to work with the IBM Event Streams Starter Application.</li> </ul> </details> <h2 id=back-to-the-cluster-configuration>Back to the Cluster configuration<a class=headerlink href=#back-to-the-cluster-configuration title="Permanent link">&para;</a></h2> <p>Event Streams cluster can be configured with Yaml and you can review the following cluster definition to explain some of the major properties: <a href=https://github.com/ibm-cloud-architecture/eda-gitops-catalog/blob/main/cp4i-operators/event-streams/operands/prod-small/eventstreams-prod.yaml>EDA GitOps Catalog - example of production cluster.yaml</a>:</p> <table> <thead> <tr> <th>Property</th> <th>Description</th> </tr> </thead> <tbody> <tr> <td><strong>Replicas</strong></td> <td>specify the # of brokers or zookeeper</td> </tr> <tr> <td><strong>Resources</strong></td> <td>CPU or mem requested and limit</td> </tr> <tr> <td><strong>Listeners</strong></td> <td>Define how to access the cluster: External with scram authentication and TLS encryption, and internal using TLS authentication or PLAIN.</td> </tr> <tr> <td><strong>Entity operators</strong></td> <td>Enable topic and user to be managed by operator</td> </tr> <tr> <td><strong>Rack awareness</strong></td> <td>To use zone attribute from node to allocate brokers in different AZ</td> </tr> <tr> <td><strong>Cruise Control</strong></td> <td>Open source for cluster rebalancing</td> </tr> <tr> <td><strong>Metrics</strong></td> <td>To export different Kafka metrics to Prometheus via JMX exporter</td> </tr> </tbody> </table> <p>For Kafka, the following aspects of a deployment can impact the resources you need:</p> <ul> <li>Throughput and size of messages</li> <li>The number of network threads handling messages</li> <li>The number of producers and consumers</li> <li>The number of topics and partitions</li> </ul> <h2 id=producing-messages>Producing messages<a class=headerlink href=#producing-messages title="Permanent link">&para;</a></h2> <p>The <a href=https://ibm.github.io/event-streams/about/producing-messages/ >product documentation - producing message section</a> goes into details of the concepts. For a demonstration purpose, you need to illustrate that you can have multiple types of Kafka producer:</p> <ul> <li>Existing Queuing apps, which are using IBM MQ, and get their messages transparently sent to Event Streams, using <a href="https://www.ibm.com/docs/en/ibm-mq/9.2?topic=scenarios-streaming-queues">IBM MQ Streaming Queue</a> and <a href=https://github.com/ibm-messaging/kafka-connect-mq-source>MQ Source Kafka Connector</a>.</li> </ul> <p><img alt src=images/streaming_queuesa.jpeg></p> <ul> <li>Microservice applications publishing events using Kafka producer API, or reactive messaging in Java Microprofile. For Nodejs, Python there is a C library which supports the Kafka APIs. We have code template for that.</li> <li>Change data capture product, like <a href=https://debezium.io/ >Debezium</a>, that gets database updates and maps records to events in topic. One topic per table. Some data transformation can be done on the fly.</li> <li>Streaming applications, that do stateful computing, real-time analytics, consuming - processing - publishing events from one to many topics and produce to one topic. </li> <li>App connect flow can also being source for events to Events Streams, via connectors.</li> </ul> <p>The following diagram illustrates those event producers.</p> <p><img alt src=images/different-producers.png></p> <p>Each producer needs to get a URL to the broker, defines the protocol to authenticate, and gets server side TLS certificate, the topic name, and that's it to start sending messages.</p> <p>For production deployment, event structures are well defined and schema are used to ensure consumer can understand how to read messages from the topic/partition. Event Streams offers a schema registry to manage those schema definitions.</p> <p>You can introduce the schema processing with the figure below:</p> <p><img alt src=images/schema-registry.png></p> <details class=-> <summary>Schema flow explanations</summary> <ul> <li>(1) Avro or Json schemas are defined in the context of a producer application. As an example you can use the <a href=https://github.com/ibm-cloud-architecture/eda-quickstarts/blob/main/quarkus-reactive-kafka-producer/src/main/avro/OrderEvent.avsc>OrderEvent.avsc in the EDA quickstart</a> project. </li> <li>They are uploaded to Schema registry, you will demonstrate that in 2 minutes</li> <li>(2) Producer application uses Serializer that get schema ID from the registry </li> <li>(3) Message includes metadata about the schema ID</li> <li>(4) So each message in a topic/partition may have a different schema ID, which help consumer to be able to process old messages</li> <li>(5) Consumers get message definitions from the central schema registry.</li> </ul> </details> <h2 id=schema-registry>Schema registry<a class=headerlink href=#schema-registry title="Permanent link">&para;</a></h2> <p>This is really an introduction to the schema management, a deeper demo will take around 35 minutes and is described in <a href=https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/schema-registry-on-ocp/ >this EDA lab</a></p> <ol> <li> <p>Get the ItemEvent schema definition (Defined in the context of the real-time inventory demo) using the command below:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=w>    </span>curl<span class=w> </span>https://raw.githubusercontent.com/ibm-cloud-architecture/refarch-eda-store-simulator/master/backend/src/main/avro/ItemEvent.avsc<span class=w> </span>&gt;<span class=w> </span>ItemEvent.avsc
</code></pre></div> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>When running on a multi-tenant Event Streams cluster you need to modify the name of the schema name, to avoid conflicting with other schema name in the registry. In the context of the <code>IBM Tech Academy</code>, we propose you prefix the name with your assigned user-id.</p> </div> </li> <li> <p>Go to the Schema registry in the Event Streams console, and click to <code>Add Schema</code></p> <p><img alt src=images/es-schema-registry.png></p> </li> <li> <p>In the <code>Add schema</code> view, select <code>Upload definition</code>, select the <code>ItemEvent.avsc</code></p> <p><img alt src=images/add-schema.png></p> </li> <li> <p>The first ItemEvent schema is validated, </p> <p><img alt src=images/item-schema-1.png></p> </li> <li> <p>You can see its definition too</p> <p><img alt src=images/item-schema-2.png></p> </li> <li> <p>Do not forget to press <code>Add schema</code> to save your work. Now the schema is visible in the registry</p> <p><img alt src=images/item-schema-3.png></p> </li> </ol> <p>Now any future producer application discussions should be around level of control of the exactly once, at most once delivery, failover and back preasure. This is more complex discussion, what is important to say is that we can enforce producer to be sure records are replicated before continuing, we can enforce avoiding record duplication, producer can do message buffering and send in batch, so a lot of controls are available depending of the application needs.</p> <details class=-> <summary>Reading more</summary> <ul> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-producers/ >Producer best practices and considerations</a></li> <li><a href=https://ibm-cloud-architecture.github.io/vaccine-solution-main/solution/orderms/ >Using the outbox pattern with Debezium and Quarkus</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/db2-debezium/ >DB2 debezium lab</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/avro-schemas/ >Playing with Avro Schema</a></li> <li><a href=https://ibm.github.io/event-streams/about/producing-messages/ >Event Streams product documentation</a></li> </ul> </details> <h2 id=consumer-application-consumer-group>Consumer application - consumer group<a class=headerlink href=#consumer-application-consumer-group title="Permanent link">&para;</a></h2> <p>Let’s take a look at consumer group and how consumer gets data from Topic/partition. The following figure will help supporting the discussion:</p> <p><img alt src=images/consumer-groups.png></p> <details class=-> <summary>Explanations</summary> <ul> <li>Consumer application define a property to group multiple instances of those application into a group.</li> <li>Topic partitions are only here to support scaling consumer processing</li> <li>Brokers are keeping information about group, offset and partition allocation to consumer </li> <li>When a consumer is unique in a group, it will get data from all partitions.</li> <li>We cannot have more consumer than number of topic, if not the consumer will do nothing</li> <li>Membership in a consumer group is maintained dynamically</li> <li>When the consumer does not send heartbeats for a duration of <code>session.timeout.ms</code>, then it is considered unresponsive and its partitions will be reassigned.</li> <li>For each consumer group, Kafka remembers the committed offset for each partition being consumed.</li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-consumers/#offset-management>Understanding offset</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-consumers/ >Get more details on consumer best practices</a></li> </ul> </details> <p>Recall that offset is just a numeric identifier of a consumer position of the last record read within a partition. Consumers periodically need to commit the offsets they have received, to present a recovery point in case of failure.</p> <p>For reliable consumers, it means the commitment of the read offset is done by code once the consumer is done with its processing. There is an important limitation within App Connect Kafka consumer node as there is no way to commit by code, so do not propose App Connect as a viable solution if you need to do not loose message. Or support adding new java custom code to do the management of offset.</p> <p>From Event Streams demonstration point of view, we can only demonstrate consumer groups for a given topic, and if consumers are behind in term of reading records from a partition.</p> <ol> <li> <p>In the Event Streams console go to the <code>Topics</code> view and <code>Consumer Groups tab</code> of one of the topic. The figure below shows that there is no active member for the consumer groups , and so one partition is not consumed by any application.</p> <p><img alt src=images/topic-vw-consumer-grp.png></p> </li> <li> <p>Another view is in the <code>Consumer Groups</code> which lists all the consumer groups that have been connected to any topic in the cluster: This view helps to assess if consumer are balanced. Selecting one group will zoom into the partition and offset position for member of the group. Offset lag is what could be a concern. The consumer lag for a partition is the difference between the offset of the most recently published message and the consumer's committed offset.</p> <p><img alt src=images/consumer-lag.png></p> <p><em>Consumer lag may show that consumers are not processing records at the same pace that producer is publishing them. This could not be a problem, until this lag is becoming too high and compaction or retention by time or size will trigger, removing old records. In this case consumers will miss messages.</em></p> </li> </ol> <details class=-> <summary>Reading more</summary> <ul> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-consumers/ >Review Consumer</a></li> <li><a href=https://ibm.github.io/event-streams/about/consuming-messages/ >Product documentation - Consuming messages</a></li> </ul> </details> <h2 id=user-management-and-security>User management and security<a class=headerlink href=#user-management-and-security title="Permanent link">&para;</a></h2> <p>There are two types of user management in Event Streams: the human users, to access the user interface and the application users to access Brokers and Kafka Topics.</p> <p>Application users are defined with KafkaUser custom resources. The Yaml also describes access control list to the topic. The <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/services/ibm-eventstreams/base/tls-user.yaml>following KafkaUser yaml file</a> is an example of application user used to authenticate with mutual TLS.</p> <p>Such user can also being created by using the <code>connect to the cluster</code> option in Event Streams console.</p> <p>The Acces Control Lists are defined by specifying the resource type and the type of operation authorized. User certificates and passwords are saved in secrets. The ACL rules define the operations allowed on Kafka resources based on the username:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=nt>acls</span><span class=p>:</span>
<a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>resource</span><span class=p>:</span>
<a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">topic</span><span class=w>   </span>
<a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=s>&#39;rt-&#39;</span>
<a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a><span class=w>    </span><span class=nt>patternType</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">prefix</span>
<a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a><span class=nt>operation</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Write</span>
<a id=__codelineno-7-7 name=__codelineno-7-7 href=#__codelineno-7-7></a><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>resource</span><span class=p>:</span>
<a id=__codelineno-7-8 name=__codelineno-7-8 href=#__codelineno-7-8></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">topic</span>
<a id=__codelineno-7-9 name=__codelineno-7-9 href=#__codelineno-7-9></a><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=s>&#39;*&#39;</span>
<a id=__codelineno-7-10 name=__codelineno-7-10 href=#__codelineno-7-10></a><span class=w>    </span><span class=nt>patternType</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">literal</span>
<a id=__codelineno-7-11 name=__codelineno-7-11 href=#__codelineno-7-11></a><span class=nt>operation</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Read</span>
<a id=__codelineno-7-12 name=__codelineno-7-12 href=#__codelineno-7-12></a><span class="p p-Indicator">-</span><span class=w> </span><span class=nt>resource</span><span class=p>:</span>
<a id=__codelineno-7-13 name=__codelineno-7-13 href=#__codelineno-7-13></a><span class=w>    </span><span class=nt>type</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">topic</span>
<a id=__codelineno-7-14 name=__codelineno-7-14 href=#__codelineno-7-14></a><span class=w>    </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class=s>&#39;*&#39;</span>
<a id=__codelineno-7-15 name=__codelineno-7-15 href=#__codelineno-7-15></a><span class=w>    </span><span class=nt>patternType</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">literal</span>
<a id=__codelineno-7-16 name=__codelineno-7-16 href=#__codelineno-7-16></a><span class=nt>operation</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">Create</span>
</code></pre></div> <p>For human authentication, users are defined using IBM Cloud Pak foundational services Identity and Access Management (IAM). Things to keep in mind:</p> <ul> <li>IAM is in Cloud Pak | Administation console. A URL like: https://cp-console.apps........ibm.com/common-nav/dashboard</li> <li> <p>Need to define a team for resources, administrator users... using he Administration console and IAM menu:</p> <p><img alt src=images/iam-team-0.png></p> <p>Define new team, with connection to an active directory / identity provider:</p> <p><img alt src=images/iam-team-1.png></p> </li> <li> <p>Any groups or users added to an IAM team with the <code>Cluster Administrator</code> or <code>Administrator</code> role can log in to the Event Streams UI and CLI</p> <p><img alt src=images/iam-team-2.png></p> <p>or non admin user:</p> <p><img alt src=images/iam-team-3.png></p> </li> <li> <p>Any groups or users with the Administrator role will not be able to log in until the namespace that contains the Event Streams cluster is added as a resource for the IAM team.</p> <p><img alt src=images/add-ns-to-iam-team.png></p> </li> <li> <p>If the cluster definition includes <code>spec.strimziOverrides.kafka.authorization: runas</code>, users are mapped to a Kafka principal </p> </li> </ul> <details class=-> <summary>Read more</summary> <ul> <li><a href=https://ibm.github.io/event-streams/security/managing-access/#assigning-access-to-users>Managing access - product documentation</a> </li> <li><a href="https://www.ibm.com/docs/en/cpfs?topic=users-managing-teams">Managing team with IAM</a></li> <li><a href=https://kafka.apache.org/documentation/#security_authz>ACL and authorization</a></li> <li><a href=https://strimzi.io/docs/operators/latest/configuring.html#type-AclRule-reference>ACLs rule schema reference </a></li> </ul> </details> <h2 id=kafka-connect>Kafka Connect<a class=headerlink href=#kafka-connect title="Permanent link">&para;</a></h2> <p>Kafka connect is used to connect external systems to Event Streams brokers. For production deployment the Kafka connect connectors run in cluster, (named distributed mode), to support automatic balancing, dynamic scaling and fault tolerance. In the figure below, we can see Kafka Connect cluster builds with 3 worker processes. The configuration if such worker is done with one file, that can be managed in your GitOps. (An example of such file is <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/services/kconnect/kafka-connect.yaml>here</a>) </p> <p><img alt src=images/connector-tasks.png></p> <p>Event Streams Operator supports custom resource to define Kafka connect cluster. Each connector is represented by another custom resource called KafkaConnector.</p> <p>When running in distributed mode, Kafka Connect uses three topics to store configuration, current offsets and status.</p> <p>Once the cluster is running, we can use custom resource to manage the connector. For example to get a MQ Source connector definition example, you can browse <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/apps/mq-source/kafka-mq-src-connector.yaml>this yaml</a> which specifies how to connect to the MQ broker and how to create records for Kafka.</p> <div class=highlight><pre><span></span><code><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">eventstreams.ibm.com/v1alpha1</span>
<a id=__codelineno-8-2 name=__codelineno-8-2 href=#__codelineno-8-2></a><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">KafkaConnector</span>
<a id=__codelineno-8-3 name=__codelineno-8-3 href=#__codelineno-8-3></a><span class=nt>metadata</span><span class=p>:</span>
<a id=__codelineno-8-4 name=__codelineno-8-4 href=#__codelineno-8-4></a><span class=w>  </span><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">mq-source</span>
<a id=__codelineno-8-5 name=__codelineno-8-5 href=#__codelineno-8-5></a><span class=w>  </span><span class=nt>labels</span><span class=p>:</span>
<a id=__codelineno-8-6 name=__codelineno-8-6 href=#__codelineno-8-6></a><span class=w>    </span><span class=nt>eventstreams.ibm.com/cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">eda-kconnect-cluster</span>
<a id=__codelineno-8-7 name=__codelineno-8-7 href=#__codelineno-8-7></a><span class=nt>spec</span><span class=p>:</span>
<a id=__codelineno-8-8 name=__codelineno-8-8 href=#__codelineno-8-8></a><span class=w>  </span><span class=nt>class</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">com.ibm.eventstreams.connect.mqsource.MQSourceConnector</span>
<a id=__codelineno-8-9 name=__codelineno-8-9 href=#__codelineno-8-9></a><span class=w>  </span><span class=nt>tasksMax</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id=__codelineno-8-10 name=__codelineno-8-10 href=#__codelineno-8-10></a><span class=w>  </span><span class=nt>config</span><span class=p>:</span>
<a id=__codelineno-8-11 name=__codelineno-8-11 href=#__codelineno-8-11></a><span class=w>    </span><span class=nt>mq.queue.manager</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">QM1</span>
<a id=__codelineno-8-12 name=__codelineno-8-12 href=#__codelineno-8-12></a><span class=w>    </span><span class=nt>mq.connection.name.list</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">store-mq-ibm-mq.rt-inventory-dev.svc</span>
<a id=__codelineno-8-13 name=__codelineno-8-13 href=#__codelineno-8-13></a><span class=w>    </span><span class=nt>mq.channel.name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">DEV.APP.SVRCONN</span>
<a id=__codelineno-8-14 name=__codelineno-8-14 href=#__codelineno-8-14></a><span class=w>    </span><span class=nt>mq.queue</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">ITEMS</span>
<a id=__codelineno-8-15 name=__codelineno-8-15 href=#__codelineno-8-15></a><span class=w>    </span><span class=nt>mq.bath.size</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">250</span>
<a id=__codelineno-8-16 name=__codelineno-8-16 href=#__codelineno-8-16></a><span class=w>    </span><span class=nt>producer.override.acks</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<a id=__codelineno-8-17 name=__codelineno-8-17 href=#__codelineno-8-17></a><span class=w>    </span><span class=nt>topic</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">items</span>
<a id=__codelineno-8-18 name=__codelineno-8-18 href=#__codelineno-8-18></a><span class=w>    </span><span class=nt>key.converter</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">org.apache.kafka.connect.storage.StringConverter</span>
<a id=__codelineno-8-19 name=__codelineno-8-19 href=#__codelineno-8-19></a><span class=w>    </span><span class=nt>value.converter</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">org.apache.kafka.connect.storage.StringConverter</span>
<a id=__codelineno-8-20 name=__codelineno-8-20 href=#__codelineno-8-20></a><span class=w>    </span><span class=nt>mq.record.builder</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">com.ibm.eventstreams.connect.mqsource.builders.DefaultRecordBuilder</span>
<a id=__codelineno-8-21 name=__codelineno-8-21 href=#__codelineno-8-21></a><span class=w>    </span><span class=nt>mq.connection.mode</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">client</span>
<a id=__codelineno-8-22 name=__codelineno-8-22 href=#__codelineno-8-22></a><span class=w>    </span><span class=nt>mq.message.body.jms</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<a id=__codelineno-8-23 name=__codelineno-8-23 href=#__codelineno-8-23></a><span class=w>    </span><span class=nt>mq.record.builder.key.header</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">JMSCorrelationID</span>
</code></pre></div> <p>to improve connector source throughput we can control the producer properties like the acknowledge level expected.</p> <p>The real time inventory demo includes MQ source connector.</p> <details class=-> <summary>Read more</summary> <ul> <li><a href=https://ibm.github.io/event-streams/connecting/connectors/ >Event Streams documentation - kafka connect</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-connect/ >Kafka Connect technology deeper dive</a></li> </ul> </details> <h2 id=monitoring>Monitoring<a class=headerlink href=#monitoring title="Permanent link">&para;</a></h2> <p>The IBM Event Streams UI provides information about the health of your environment at a glance. In the bottom right corner of the UI, a message shows a summary status of the system health.</p> <ul> <li>Using the JMX exporter, you can collect JMX metrics from Kafka brokers, ZooKeeper nodes, and Kafka Connect nodes, and export them to Prometheus</li> </ul> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>Be aware IBM Cloud Pak foundational services 3.8 and later does not include Prometheus so you will get <a href=https://ibm.github.io/event-streams/troubleshooting/metrics-not-available/ >Event Streams metrics not available</a> error message. On Biggs as of 04/19/22, the cluster configuration was done. If you need to do it on your cluster see those two files: <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/services/ibm-eventstreams/base/cluster-monitoring-cm.yaml>cluster-monitoring-cm.yaml</a> to enable user workload monitoring with Prometheus and <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/services/ibm-eventstreams/base/pod-monitors.yaml>pod-monitors.yaml</a> to declare the <a href=https://docs.openshift.com/container-platform/4.9/rest_api/monitoring_apis/podmonitor-monitoring-coreos-com-v1.html>PodMonitor</a> to define scrapeable endpoints of a Kubernetes Pod serving Prometheus metrics.</p> </div> <ol> <li> <p>Assess Event Streams cluster state: Go to the project where the cluster runs, select one of the Kafka Pod. You can see the pod via the OpenShift workloads menu, or by using the Event Streams Operator &gt; Resources and then filter on pods:</p> <p><img alt src=images/filter-on-pods.png></p> <p>Select one of the pods and go to the metrics to see memory, CPU, network and filesystem usage metrics.</p> <p><img alt src=images/kafka-pod-metrics.png></p> </li> <li> <p>Access the Cloud Pak | Administration console to select Monitoring</p> <p><img alt src=images/cp4i-monitoring.png></p> </li> <li> <p>Switch organization to select where <code>Event Streams</code> is running</p> <p><img alt src=images/switch-org-to-es.png></p> </li> <li> <p>Then go to the grafana Dashboard menu on the left &gt; Manage and select event streams dashboard</p> <p><img alt src=images/grafana-dashboard-es.png></p> </li> <li> <p>In the Grafana dashboard select the namespace for event streams (e.g. <code>cp4i-eventstreams</code>), the cluster name (<code>es-demo</code>), the brokers, and the topic to monitor. </p> <p><img alt src=images/es-grafana-dashboard.png></p> </li> </ol> <details class=-> <summary>More reading</summary> <ul> <li><a href=https://ibm.github.io/event-streams/administering/deployment-health/ >Product documentation</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-monitoring/ >EDA monitoring study</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/monitoring-on-ocp/ >Event Streams Monitoring on OpenShift lab</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/monitoring-on-ocp/#create-an-alert>Creating alert from Prometheus</a></li> </ul> </details> <h2 id=event-streaming>Event Streaming<a class=headerlink href=#event-streaming title="Permanent link">&para;</a></h2> <h3 id=kafka-streams>Kafka Streams<a class=headerlink href=#kafka-streams title="Permanent link">&para;</a></h3> <p>Kafka Streams is client API to build microservices with input and output data are in Kafka. It is based on programming a graph of processing nodes to support the business logic developer wants to apply on the event streams.</p> <h3 id=apache-flink-as-your-streaming-platform>Apache Flink as your streaming platform<a class=headerlink href=#apache-flink-as-your-streaming-platform title="Permanent link">&para;</a></h3> <p>To be done.</p> <h2 id=real-time-inventory-demo>Real-time inventory demo<a class=headerlink href=#real-time-inventory-demo title="Permanent link">&para;</a></h2> <p>It is possible to propose a more complex solution to illustrate modern data pipeline using components like MQ source Kafka Connector, Kafka Streams implementation and Cloud Object Storage sink, Elastic Search and Pinot.</p> <p>This scenario implements a simple real-time inventory management solution based on some real life MVPs we developed in 2020. For a full explanation of the use case and scenario demo go to <a href=https://ibm-cloud-architecture.github.io/eda-rt-inventory-gitops/demo-script/#real-time-inventory-scenario-presentation>this chapter</a> in EDA reference architecture.</p> <p><img alt src=images/mq-es-demo.png></p> <p>The solution can be deployed using few commands or using GitOps. </p> <p>See <a href=../lab3/ >Lab3-4</a></p> <details class=-mk> <summary>More Reading</summary> <ul> <li><a href=https://ibm-cloud-architecture.github.io/eda-rt-inventory-gitops/demo-script/#real-time-inventory-scenario-presentation>Description of the scenario and demo script</a></li> <li><a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops>GitOps project to deploy the solution</a></li> <li><a href=https://github.com/ibm-cloud-architecture/eda-gitops-catalog>EDA GitOps Catalog to deploy Cloud Pak for Integration operators</a></li> </ul> </details> <h2 id=geo-replication>Geo-replication<a class=headerlink href=#geo-replication title="Permanent link">&para;</a></h2> <p>We will go over two main concepts: replication to a passive and active Event Streams cluster. Geo Replication is the IBM packaging of Mirror Maker 2. </p> <h3 id=demonstrating-geo-replication>Demonstrating Geo Replication<a class=headerlink href=#demonstrating-geo-replication title="Permanent link">&para;</a></h3> <p>The geo-replication feature creates copies of your selected topics to help with disaster recovery.</p> <h3 id=mirror-maker-2>Mirror Maker 2<a class=headerlink href=#mirror-maker-2 title="Permanent link">&para;</a></h3> <p>Mirror Maker 2 is a Kafka Connect framework to replicate data between different Kafka Cluster, so it can be used between Event Streams clusters, but also between Confluent to/from Event Streams, Kafka to/from Event Streams...</p> <p>The following diagram can be used to present the MM2 topology</p> <p><img alt src=images/mm2-topology.png></p> <h3 id=active-passive>Active - Passive<a class=headerlink href=#active-passive title="Permanent link">&para;</a></h3> <p>See a demonstration for the real-time inventory and replication in <a href=https://ibm-cloud-architecture.github.io/eda-rt-inventory-gitops/mm2/ >this article</a></p> <p><img alt src=images/mm2-dr.png></p> <h3 id=active-active>Active - Active<a class=headerlink href=#active-active title="Permanent link">&para;</a></h3> <p><img alt src=images/mm2-act-act.png></p> <details class=-> <summary>Read more</summary> <ul> <li><a href=https://ibm.github.io/event-streams/georeplication/about/ >Geo Replication - Product documentation</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/technology/kafka-mirrormaker/ >EDA techno overview for Mirror Maker 2</a></li> <li><a href=https://ibm-cloud-architecture.github.io/eda-rt-inventory-gitops/mm2/ >Demonstration in the context of real-time inventory</a></li> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/kafka-mm2/ >EDA lab on mirror maker 2</a></li> </ul> </details> <h2 id=day-2-operations>Day 2 operations<a class=headerlink href=#day-2-operations title="Permanent link">&para;</a></h2> <p>In this section, you should be able to demonstrate some of the recurring activities, operation team may perform for the Event Streams and OpenShift platform for maintenance:</p> <ul> <li>Change Cluster configuration</li> <li>Add topic or change topic configuration like adding partition</li> </ul> <h3 id=gitops>GitOps<a class=headerlink href=#gitops title="Permanent link">&para;</a></h3> <p>The core idea of GitOps is having a Git repository that always contains declarative descriptions of the infrastructure currently desired in the production environment and an automated process to make the production environment matches the described state in the repository. Git is the source of truth for both application code, application configuration, dependant service/product deployments, infrastructure config and deployment.</p> <p>In the following figure, we just present the major components that will be used to support GitOps and day 2 operations: </p> <p><img alt=gitops src=images/gitops.png width=80%></p> <details class=-> <summary>Explanations</summary> <ul> <li>cluster configuration, topics, users ACL are defined as yaml resources in the solution GitOps. <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/tree/main/environments/rt-inventory-stage/services/ibm-eventstreams/base/eventstreams-prod.yaml>Cluster example for prod</a></li> <li>(1) Operator definitions are also defined in the gitops and then change to the version subscription will help do product upgrade. <a href=https://github.com/ibm-cloud-architecture/eda-gitops-catalog/blob/main/cp4i-operators/event-streams/operator/base/subscription.yaml>Event Streams subscription</a> with <a href=https://github.com/ibm-cloud-architecture/eda-gitops-catalog/tree/main/cp4i-operators/event-streams/operator/overlays/v2.5>overlays</a> for a new version.</li> <li>(2) ArgoCD apps are <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/config/argocd/rt-inventory-dev-es-services-app.yaml>defined in the GitOps</a> and then once running on the GitOps Server, will monitor changes to the source gitOps content </li> <li>(3) when change occurs, the underlying kubernetes resources are modified</li> <li>(4) Operator maintains control to the runtime pods according to the modified manifest</li> </ul> </details> <p>Cloud Pak for integration, event streams, MQ, API Connect operators help to support deployment as a Day 1 operation, but also support maintenance or Day 2 operations. Operator is constantly watching your cluster’s desired state for the software installed and act on them.</p> <p>Using a GitOps approach, we can design a high-level architecture view for the deployment of all the event-driven solution components: as in previous figure, operators, ArgoCD apps, cluster, topics... are defined in the solution gitops and then the apps deployment, config map, service, secrets, routes are also defined according to the expected deployment model.</p> <p><img alt="GitOps HL View" src=images/gitops-hl-view.png width=100%></p> <p>In the figure above, the dev, and staging projects have their own Event Streams clusters. Production is in a separate OpenShift Cluster and event streams cluster is multi-tenant.</p> <p>We are using a special Git repository to manage a catalog of operator definitions/ subscriptions. This is the goal of the <a href=https://github.com/ibm-cloud-architecture/eda-gitops-catalog>eda-gitops-catalog repository</a>.</p> <p>A solution will have a specific gitops repository that manages services (operands) and application specifics deployment manifests.</p> <h3 id=start-the-gitops-demo>Start the GitOps demo<a class=headerlink href=#start-the-gitops-demo title="Permanent link">&para;</a></h3> <div class="admonition warning"> <p class=admonition-title>Warning</p> <p>In the context of the <strong>Tech academy</strong>, if you want to use Gitops you should use the <a href=../lab4/ >lab 4 exercise</a> as it is a little bit simpler than to execute next section.</p> </div> <p>To be able to do continuous deployment we need to have some ArgoCD apps deployed on GitOps server. In all gitOps demos, we assume you have a fork of the <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops>eda-rt-inventory-gitops</a>. </p> <p>If you are not using a cluster with Event Streams already installed in the <code>cp4i-eventstreams</code>, you may need to modify the <a href>Copy Secret job ()</a> so it can get the <code>ibm-entitlement-key</code> from the good namespace.</p> <ol> <li> <p>If not done yet, jumpstart GitOps</p> <p><div class=highlight><pre><span></span><code><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>oc<span class=w> </span>apply<span class=w> </span>-k<span class=w> </span>demo/argocd
</code></pre></div> 1. Access to the ArgoCD console</p> <p><div class=highlight><pre><span></span><code><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a><span class=w> </span>chrome<span class=w> </span>https://<span class=k>$(</span>oc<span class=w> </span>get<span class=w> </span>route<span class=w> </span>openshift-gitops-server<span class=w> </span>-o<span class=w> </span><span class=nv>jsonpath</span><span class=o>=</span><span class=s1>&#39;{.status.ingress[].host}&#39;</span><span class=w>  </span>-n<span class=w> </span>openshift-gitops<span class=k>)</span>
</code></pre></div> 1. User is <code>admin</code> and password is the result of </p> <div class=highlight><pre><span></span><code><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a>oc<span class=w> </span>extract<span class=w> </span>secret/openshift-gitops-cluster<span class=w> </span>-n<span class=w> </span>openshift-gitops<span class=w> </span>--to<span class=o>=</span>-
</code></pre></div> </li> <li> <p>You should have two apps running in the default scope/ project.</p> <p><img alt src=images/demo-argo.png></p> <p>The argocd apps are monitoring the content of the demo/ env folder and once deployed, you should have a simple Event Streams node with one zookeeper under the project <code>es-demo-day2</code>.</p> </li> </ol> <h3 id=event-streams-cluster-definition-with-gitops>Event Streams cluster definition with GitOps<a class=headerlink href=#event-streams-cluster-definition-with-gitops title="Permanent link">&para;</a></h3> <p>The goal of this section is to demonstrate how to define an Event Stream cluster with configuration and change the number of replicas. This is a very simple use case to try to use the minimum resources. So the basic cluster definition use 1 broker and 1 zookeeper. The file is <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/demo/env/es-demo.yaml>es-demo.yaml</a>. it is using Event Streams version 10.5.0 and one replicas</p> <ol> <li> <p>In GitOps console, if you go to the demo-env app, you will see there is one Kafka broker and also a lot of Kubernetes resources defined</p> <p><img alt src=images/basic-es-gitops.png></p> </li> <li> <p>In the <code>es-demo-day2</code> project, use <code>oc get pods</code> to demonstrate the number of brokers</p> <div class=highlight><pre><span></span><code><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>NAME<span class=w>                                    </span>READY<span class=w>   </span>STATUS<span class=w>      </span>RESTARTS<span class=w>   </span>AGE
<a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>cpsecret-48k2c<span class=w>                          </span><span class=m>0</span>/1<span class=w>     </span>Completed<span class=w>   </span><span class=m>0</span><span class=w>          </span>11m
<a id=__codelineno-12-3 name=__codelineno-12-3 href=#__codelineno-12-3></a>demo-entity-operator-558c94dc57-lxp6s<span class=w>   </span><span class=m>3</span>/3<span class=w>     </span>Running<span class=w>     </span><span class=m>0</span><span class=w>          </span>24m
<a id=__codelineno-12-4 name=__codelineno-12-4 href=#__codelineno-12-4></a>demo-ibm-es-admapi-6678c47b95-hg82v<span class=w>     </span><span class=m>1</span>/1<span class=w>     </span>Running<span class=w>     </span><span class=m>0</span><span class=w>          </span>14m
<a id=__codelineno-12-5 name=__codelineno-12-5 href=#__codelineno-12-5></a>demo-ibm-es-metrics-b974c7585-jpfc7<span class=w>     </span><span class=m>1</span>/1<span class=w>     </span>Running<span class=w>     </span><span class=m>0</span><span class=w>          </span>14m
<a id=__codelineno-12-6 name=__codelineno-12-6 href=#__codelineno-12-6></a>demo-kafka-0<span class=w>                            </span><span class=m>1</span>/1<span class=w>     </span>Running<span class=w>     </span><span class=m>0</span><span class=w>          </span>24m
<a id=__codelineno-12-7 name=__codelineno-12-7 href=#__codelineno-12-7></a>demo-zookeeper-0<span class=w>                        </span><span class=m>1</span>/1<span class=w>     </span>Running<span class=w>     </span><span class=m>0</span><span class=w>          </span>25m
</code></pre></div> </li> <li> <p>Modify the number of replicas</p> <div class=highlight><pre><span></span><code><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a><span class=w>    </span><span class=nt>kafka</span><span class=p>:</span>
<a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a><span class=w>        </span><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
</code></pre></div> </li> <li> <p>Commit and push your changes to your git repository and see ArgoCD changing the configuration, new pods should be added.</p> </li> <li> <p>You can enforce a refresh to get update from Git and then navigate the resources to see the new brokers added (demo-kafka-1):</p> <p><img alt src=images/es-2-brokers.png></p> </li> </ol> <p>Adding a broker will generate reallocation for topic replicas.</p> <h3 id=event-streams-cluster-upgrade>Event Streams Cluster upgrade<a class=headerlink href=#event-streams-cluster-upgrade title="Permanent link">&para;</a></h3> <p>This will be difficult to demonstrate but the flow can be explain using the OpenShift Console.</p> <ol> <li>First you need to be sure the cloud pak for integration services are upgraded. (See <a href="https://www.ibm.com/docs/en/cloud-paks/cp-integration/2021.4?topic=upgrading">this note</a>)</li> <li> <p>Two things to upgrade in this order: Event Streams operator and then the cluster instances.</p> <p>You can upgrade the Event Streams operator to version 2.5.2 directly from version 2.5.x, 2.4.x, 2.3.x, and 2.2.x..</p> <p>You can upgrade the Event Streams operand to version 11.0.0 directly from version 10.5.0, 10.4.x</p> </li> <li> <p>Start by presenting the version of an existing running Cluster definition</p> <p><img alt src=images/dev-es-10.4.png></p> </li> <li> <p>May be show some messages in a topic, for example the Store Simulator may have sent messages to the <code>items</code> topic</p> </li> <li> <p>Go to the <code>openshift-operators</code> and select the event streams operator, explain the existing chaneel then change the channel number</p> <p><img alt src=images/es-operator-10.4.png></p> <p><img alt src=images/es-channel-v25.png></p> <ul> <li>Event Streams instance must have more than one ZooKeeper node or have persistent storage enabled.</li> <li>Upgrade operator is by changing the channel in the operator subscription. All Event Streams pods that need to be updated as part of the upgrade will be gracefully rolled. Where required ZooKeeper pods will roll one at a time, followed by Kafka brokers rolling one at a time.</li> </ul> </li> <li> <p>Update the cluster definition version to new version (10.5.0 in below screen shot), thne this will trigger zookeeper and kafka broker update.</p> <p><img alt src=images/es-upgrade.png></p> <h3 id=topic-management-with-gitops>Topic management with GitOps<a class=headerlink href=#topic-management-with-gitops title="Permanent link">&para;</a></h3> </li> </ol> <p>The goal of this section is to demonstrate how to change topic definition using Argocd and git.</p> <ol> <li> <p>Modify the file <a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/demo/env/es-topic.yaml>es-topic.yaml</a> by adding a new topic inside this file with the following declaration:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a><span class=nt>apiVersion</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">eventstreams.ibm.com/v1beta1</span>
<a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a><span class=nt>kind</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">KafkaTopic</span>
<a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a><span class=nt>metadata</span><span class=p>:</span>
<a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a><span class=nt>name</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">demo-2-topic</span>
<a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a><span class=nt>labels</span><span class=p>:</span>
<a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a><span class=w>    </span><span class=nt>eventstreams.ibm.com/cluster</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">demo</span>
<a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a><span class=nt>spec</span><span class=p>:</span>
<a id=__codelineno-14-8 name=__codelineno-14-8 href=#__codelineno-14-8></a><span class=nt>partitions</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class=w> </span>
<a id=__codelineno-14-9 name=__codelineno-14-9 href=#__codelineno-14-9></a><span class=nt>replicas</span><span class=p>:</span><span class=w> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
</code></pre></div> </li> <li> <p>Commit and push your changes to your git repository and see ArgoCD changing the configuration, new pods should be added.</p> </li> <li>Do <code>oc get kafkatopics</code> or go to Event Streams operator in the <code>es-demo-day2</code> project to see all the Event Streams component instances.</li> </ol> <h3 id=repartitioning>Repartitioning<a class=headerlink href=#repartitioning title="Permanent link">&para;</a></h3> <ol> <li> <p>You can demonstrate how to change the number of partition for an existing topic (<code>rt-items</code>) from 3 to 5 partitions:</p> <div class=highlight><pre><span></span><code><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a>oc<span class=w> </span>apply<span class=w> </span>-f<span class=w> </span>environments/rt-inventory-dev/services/ibm-eventstreams/base/update-es-topic.yaml
</code></pre></div> </li> <li> <p>Add more instances on the consumer part: taking the <code>store-aggregator</code> app and add more pods from the deployment view in OpenShift console, or change the deployment.yaml descriptor and push the change to the git repository so GitOps will catch and change the configuration:</p> <p><img src=./images/ alt></p> <div class=highlight><pre><span></span><code><a id=__codelineno-16-1 name=__codelineno-16-1 href=#__codelineno-16-1></a><span class=c1># modify https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops/blob/main/environments/rt-inventory-dev/apps/store-inventory/services/store-inventory/base/config/deployment.yaml</span>
<a id=__codelineno-16-2 name=__codelineno-16-2 href=#__codelineno-16-2></a>oc<span class=w> </span>apply<span class=w> </span>-f<span class=w> </span>environments/rt-inventory-dev/apps/store-inventory/services/store-inventory/base/config/deployment.yaml
</code></pre></div> </li> </ol> <details class=-> <summary>Read more</summary> <ul> <li><a href=https://ibm.github.io/event-streams/getting-started/using-kafka-console-tools/ >Event Streams doc on Kafka tools mapping to cli</a></li> </ul> </details> <h3 id=clean-your-gitops>Clean your gitops<a class=headerlink href=#clean-your-gitops title="Permanent link">&para;</a></h3> <ul> <li>Remove the ArgoCD apps</li> </ul> <div class=highlight><pre><span></span><code><a id=__codelineno-17-1 name=__codelineno-17-1 href=#__codelineno-17-1></a>oc<span class=w> </span>delete<span class=w> </span>-k<span class=w> </span>demo/argocd<span class=w> </span>
</code></pre></div> <ul> <li>Remove resources</li> </ul> <div class=highlight><pre><span></span><code><a id=__codelineno-18-1 name=__codelineno-18-1 href=#__codelineno-18-1></a><span class=w> </span>oc<span class=w> </span>delete<span class=w> </span>-k<span class=w> </span>demo/env
</code></pre></div> <details class=-> <summary>Read more</summary> <ul> <li><a href=https://ibm-cloud-architecture.github.io/refarch-eda/use-cases/gitops/ >Event driven solution with GitOps</a></li> <li><a href=https://github.com/ibm-cloud-architecture/eda-gitops-catalog>EDA GitOps Catalog</a></li> <li><a href=https://github.com/ibm-cloud-architecture/eda-rt-inventory-gitops>Real time inventory demo gitops</a></li> </ul> </details> <h2 id=openshift-cluster-version-upgrade>OpenShift Cluster version upgrade<a class=headerlink href=#openshift-cluster-version-upgrade title="Permanent link">&para;</a></h2> <p>There may be some questions around how to migrate a version for OCP. </p> <h3 id=principles>Principles<a class=headerlink href=#principles title="Permanent link">&para;</a></h3> <p>For clusters with internet accessibility, Red Hat provides over-the-air updates through an OpenShift Container Platform update service as a hosted service located behind public APIs.</p> <p>Due to fundamental Kubernetes design, all OpenShift Container Platform updates between minor versions must be serialized.</p> <h3 id=what-can-be-demonstrated>What can be demonstrated<a class=headerlink href=#what-can-be-demonstrated title="Permanent link">&para;</a></h3> <p>At the demo level, you can go to the Administration console in <code>Administration &gt; Cluster Settings</code> you get something like this:</p> <p><img alt src=images/ocp-cluster-setting.png></p> <ul> <li> <p>If you want to upgrade version within the same release</p> <p><img alt src=images/ocp-update-version.png></p> </li> <li> <p>Or upgrade release change the Channel version:</p> <p><img alt src=images/ocp-upgrade-release.png></p> </li> </ul> <p>As the operation will take sometime, it is not really demonstrable easily. </p> <details class=-> <summary>Read more</summary> <ul> <li><a href=https://docs.openshift.com/container-platform/4.9/updating/understanding-upgrade-channels-release.html>Openshift understanding upgrade channels release</a></li> <li><a href=https://docs.openshift.com/container-platform/4.9/updating/updating-cluster-within-minor.html#update-using-custom-machine-config-pools-canary_updating-cluster-within-minor>Canary rollout</a></li> </ul> </details> </article> </div> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.code.annotate", "content.tooltips", "navigation.tabs", "navigation.instant", "navigation.sections", "navigation.expand", "navigation.tabs.sticky"], "search": "../assets/javascripts/workers/search.e5c33ebb.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../assets/javascripts/bundle.51d95adb.min.js></script> </body> </html>